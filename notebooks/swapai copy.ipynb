{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4478529",
   "metadata": {},
   "outputs": [],
   "source": [
    "from system_prompt import DEFAULT_SYSTEM_PROMPT\n",
    "\n",
    "from dotenv import load_dotenv \n",
    "\n",
    "from typing import Annotated, Sequence, List, Optional\n",
    "from typing_extensions import TypedDict\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, SystemMessage, ToolMessage\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.tools import tool\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.prebuilt import ToolNode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8806b6c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "80070b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"system_prompt\": DEFAULT_SYSTEM_PROMPT,\n",
    "    \"temperature\": 0.9,\n",
    "    \"max_output_tokens\": 1008,\n",
    "    \"model\": \"gemini-2.5-flash-lite\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf1fb06b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentState(TypedDict):\n",
    "    \"\"\"Represents the state of the agent's workflow.\"\"\"\n",
    "    # A list of messages, annotated with a reducer to append new messages\n",
    "    messages: Annotated[Sequence[BaseMessage], add_messages]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4e96f7af",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InstructionStep(TypedDict):\n",
    "    title: str\n",
    "    details: List[str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6a9c4847",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tools\n",
    "\n",
    "@tool\n",
    "def swap_tokens(amount: float, from_token: str, to_token: str, slippage: float = 1.0):\n",
    "    \"\"\"Produce a mock swap of tokens following the instructions in 'DEFAULT_SYSTEM_PROMPT'\"\"\"\n",
    "    return {\n",
    "        \"status\": \"success\",\n",
    "        \"tx_hash\": \"0xmockswap123\",\n",
    "        \"details\": f\"Swapped {amount} {from_token} to {to_token} with {slippage}% slippage\"\n",
    "    }\n",
    "\n",
    "# Mock tool to simulate sending tokens\n",
    "@tool\n",
    "def send_tokens(amount: float, token: str, recipient: str):\n",
    "    \"\"\"Produce a mock send transaction following the instructions in 'DEFAULT_SYSTEM_PROMPT '\"\"\"\n",
    "    return {\n",
    "        \"status\": \"success\",\n",
    "        \"tx_hash\": \"0xmocksend456\",\n",
    "        \"details\": f\"Sent {amount} {token} to {recipient}\"\n",
    "    }\n",
    "\n",
    "@tool\n",
    "def give_instructions(\n",
    "    title: str,\n",
    "    summary: str,\n",
    "    steps: List[InstructionStep],\n",
    "    notes: Optional[List[str]] = None,\n",
    "    warnings: Optional[List[str]] = None,\n",
    "    checklist: Optional[List[str]] = None,\n",
    ") -> str:\n",
    "    \"\"\"Generate step-by-step instructions in markdown format with optional notes, warnings, and checklist.\"\"\"\n",
    "    \n",
    "    md = f\"## {title}\\n\"\n",
    "    md += f\"**Short summary:** {summary}\\n\\n\"\n",
    "    md += \"### Steps\\n\"\n",
    "    for i, step in enumerate(steps, 1):\n",
    "        md += f\"{i}. **{step['title']}**\\n\"\n",
    "        for d in step[\"details\"]:\n",
    "            md += f\"   - {d}\\n\"\n",
    "        md += \"\\n\"\n",
    "\n",
    "    if notes:\n",
    "        for n in notes:\n",
    "            md += f\"> **Note:** {n}\\n\"\n",
    "    if warnings:\n",
    "        for w in warnings:\n",
    "            md += f\"> **Warning:** {w}\\n\"\n",
    "    if checklist:\n",
    "        md += \"\\n**Checklist**\\n\"\n",
    "        for c in checklist:\n",
    "            md += f\"âœ… {c}\\n\"\n",
    "\n",
    "    return md\n",
    "\n",
    "tools = [swap_tokens, send_tokens, give_instructions]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7ac6c73b",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=config[\"model\"],\n",
    "    temperature=config[\"temperature\"],\n",
    "    max_output_tokens=config[\"max_output_tokens\"],\n",
    "    model_kwargs={\"system_instruction\": config[\"system_prompt\"]}\n",
    ").bind_tools(tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "29d1381b",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_CONTEXT = 16\n",
    "\n",
    "def swap_agent(state: AgentState) -> AgentState:\n",
    "    # Always include system prompt at the start\n",
    "    system_prompt = SystemMessage(content=DEFAULT_SYSTEM_PROMPT)\n",
    "\n",
    "    # Keep only the last N messages\n",
    "    messages = list(state[\"messages\"])[-MAX_CONTEXT:]\n",
    "\n",
    "    # Call LLM with system prompt + trimmed history\n",
    "    response = llm.invoke([system_prompt] + messages)\n",
    "\n",
    "    # Just return updated messages (response may be normal text or tool call)\n",
    "    return {\"messages\": messages + [response]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fc14dba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def should_continue(state: AgentState):\n",
    "    messages = state[\"messages\"]\n",
    "    last_message = messages[-1]\n",
    "    if not last_message.tool_calls:\n",
    "        return \"end\"\n",
    "    else: \n",
    "        return \"continue\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cf38d748",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new graph with the defined state\n",
    "graph = StateGraph(AgentState)\n",
    "\n",
    "# Add nodes\n",
    "graph.add_node(\"swap_agent\", swap_agent)\n",
    "\n",
    "tool_node = ToolNode(tools=tools)\n",
    "graph.add_node(\"tools\", tool_node)\n",
    "\n",
    "# Set entry point\n",
    "graph.set_entry_point(\"swap_agent\")\n",
    "\n",
    "# Define edges\n",
    "\n",
    "# The llm node can go to tool or end based on the condition\n",
    "graph.add_conditional_edges(\n",
    "    \"swap_agent\",\n",
    "    should_continue,\n",
    "    {\n",
    "        \"continue\": \"tools\",   # if tool call, go to tools\n",
    "        \"end\": END     # else just end (LLM reply is final)\n",
    "    },\n",
    ")\n",
    "\n",
    "\n",
    "graph.add_edge(\"tools\", \"swap_agent\")\n",
    "\n",
    "# compile the graph\n",
    "app = graph.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5021c3b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_with_agent(user_input: str, history=None):\n",
    "    if history is None:\n",
    "        history = []\n",
    "    \n",
    "    history.append(HumanMessage(content=user_input))\n",
    "    state = {\"messages\": history}\n",
    "    result = app.invoke(state)\n",
    "    history.extend([m for m in result[\"messages\"] if isinstance(m, AIMessage)])\n",
    "    \n",
    "    return history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6fae0505",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI: I can help you swap tokens or send tokens on the Base network. Just tell me what you need! \n",
      "AI: Got it! I can help with that.\n",
      "\n",
      "What is the amount of USDC you'd like to send, and what's the recipient's wallet address?\n",
      "AI: I've sent 5 USDC to 0xcccgtksfrj. You can view the transaction details [here](https://example.com/tx/0xmocksend456).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kamiye\\Desktop\\swap-agent\\.venv\\Lib\\site-packages\\langchain_google_genai\\chat_models.py:2007: UserWarning: HumanMessage with empty content was removed to prevent API error\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI:  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kamiye\\Desktop\\swap-agent\\.venv\\Lib\\site-packages\\langchain_google_genai\\chat_models.py:2007: UserWarning: HumanMessage with empty content was removed to prevent API error\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kamiye\\Desktop\\swap-agent\\.venv\\Lib\\site-packages\\langchain_google_genai\\chat_models.py:2007: UserWarning: HumanMessage with empty content was removed to prevent API error\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI: I'm sorry, I cannot fulfill this request. The transaction could not be completed. Please try again.\n"
     ]
    }
   ],
   "source": [
    "history = []\n",
    "while True:\n",
    "    user_input = input(\"You: \")\n",
    "    if user_input.lower() in [\"exit\", \"quit\"]:\n",
    "        break\n",
    "    history = chat_with_agent(user_input, history)\n",
    "    print(\"AI:\", history[-1].content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765c8a33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e6252f12",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ee59d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
