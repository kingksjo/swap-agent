{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "366d9b7a",
   "metadata": {},
   "source": [
    "SwapAI is an AI-powered agent that helps users perform token swaps and transfers on the Base blockchain. It takes natural language requests like *“Swap 0.5 ETH to USDC with 1% slippage”* or *“Send 20 USDC to 0xabc…”*, extracts the parameters, and executes the right action using integrated tools. The agent can also handle general user questions beyond swaps and sends.\n",
    "\n",
    "\n",
    "#### **The goal**\n",
    "\n",
    "* Make blockchain interactions easier through conversational AI.\n",
    "* Ensure clarity by confirming parameters before execution.\n",
    "* Support structured outputs (markdown) for front-end rendering.\n",
    "* Balance between tool usage (for swaps/sends) and general reasoning (for normal questions).\n",
    "\n",
    "---\n",
    "\n",
    "#### **Stack**\n",
    "\n",
    "* **Python + Jupyter Notebook** (development & testing)\n",
    "* **LangChain** (LLM orchestration, tools binding)\n",
    "* **LangGraph** (state management, branching logic, tool handling)\n",
    "* **Google Generative AI (Gemini)** or **OpenAI GPT** (LLM backbone)\n",
    "* **dotenv** (manage API keys and environment variables)\n",
    "\n",
    "---\n",
    "\n",
    "#### **Why LangChain**\n",
    "\n",
    "* Provides a clean interface to connect LLMs with tools.\n",
    "* Offers message-based state management through LangGraph.\n",
    "* Makes it easier to maintain context windows and structured workflows.\n",
    "* Prebuilt utilities like `ToolNode` and `add_messages` reduce boilerplate.\n",
    "* Extensible: easy to add new tools, memory, or different LLM backends.\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "637c43d3",
   "metadata": {},
   "source": [
    "#### Imports\n",
    "\n",
    "1. ***from system_prompt import DEFAULT_SYSTEM_PROMPT***  \n",
    "This imports the DEFAULT_SYSTEM_PROMPT from the system_prompt.py file in the repo.\n",
    "\n",
    "2. ***from dotenv import load_dotenv***  \n",
    "Loads environment variables from a .env file into your Python environment. This lets us keep API keys outside the code in a .env file with *GOOGLE_API_KEY=your API key* as its content\n",
    "\n",
    "3. ***from typing import Annotated, Sequence, List, Optional***    \n",
    "The typing module is used for type hunts.  \n",
    "\n",
    "    **Annotated** adds metadata to a type.  \n",
    "\n",
    "    **Sequence** means any ordered collection.    \n",
    "\n",
    "    **List** is specifically a python list.  \n",
    "\n",
    "    **Optional** means the value can be None.   \n",
    "\n",
    "4. ***from typing_extensions import TypedDict***  \n",
    "    **TypedDict** lets you define dictionary “schemas” with fixed keys and types.\n",
    "\n",
    "5. ***from langgraph.graph import StateGraph, END***  \n",
    "\n",
    "    **StateGraph** is the main component for building a state machine or a flowchart for an AI application.\n",
    "\n",
    "    **END** is a special, built-in node that signifies the end of a process.\n",
    "\n",
    "    StateGraph is used to orchestrate complex, multi-step workflows for an AI agent, and END is how you tell the workflow to stop.\n",
    "\n",
    "6. ***from langchain_core.messages import AIMessage, HumanMessage, SystemMessage***  \n",
    "These are data structures that represent the different roles in a conversation. They provide the structured format that chat models require to understand the flow and context of a conversation.  \n",
    "\n",
    "    **BaseMessage:** The base class for all chat messages in LangChain.  \n",
    "\n",
    "    *Subclasses:*  \n",
    "\n",
    "        **HumanMessage:** Represents input from the end-user. \n",
    "\n",
    "        **AIMessage:** Represents a response generated by the AI model.\n",
    "\n",
    "        **SystemMessage:** Sets the overall behavior, persona, or high-level instructions for the AI. \n",
    "\n",
    "        **ToolMessage:** Stores a tool's response. Needed so the conversation history includes not just what the AI says, but also what tools returned.\n",
    "\n",
    "7. ***from langchain_google_genai import ChatGoogleGenerativeAI***  \n",
    "This is the LangChain wrapper for Google's Gemini family of models. It allows the code to commmunicate with the Gemini API in a standardised way that fits into the LangChain ecosystem.\n",
    "\n",
    "8. ***from langchain_core.tools import tool***  \n",
    "This turns a python functioon into a tool that the AI can use. \n",
    "\n",
    "9. ***from langgraph.graph.message import add_messages***  \n",
    "    **add_messages** is a reducer function. This means: every time a node returns new messages, instead of replacing the old ones, it appends them to the state automatically.\n",
    "\n",
    "10. ***from langgraph.prebuilt import ToolNode***  \n",
    "    **ToolNode** is a prebuilt node that knows how to call tools. You pass it your tools list, and it manages execution + returning results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4478529",
   "metadata": {},
   "outputs": [],
   "source": [
    "from system_prompt import DEFAULT_SYSTEM_PROMPT\n",
    "\n",
    "from dotenv import load_dotenv \n",
    "\n",
    "from typing import Annotated, Sequence, List, Optional\n",
    "from typing_extensions import TypedDict\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, SystemMessage, ToolMessage\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.tools import tool\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.prebuilt import ToolNode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8806b6c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()   # This loads the .env file which contains the API key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e3d7b9b",
   "metadata": {},
   "source": [
    "#### Config Dictionary  \n",
    "Defines system prompt, creativity (temperature), response length, and model.  \n",
    "\n",
    "Keeps settings in one place → easier to tweak, cleaner code, and easier to switch models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "80070b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"system_prompt\": DEFAULT_SYSTEM_PROMPT,\n",
    "    \"temperature\": 0.9,\n",
    "    \"max_output_tokens\": 1008,\n",
    "    \"model\": \"gemini-2.5-flash-lite\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0774a0e",
   "metadata": {},
   "source": [
    "#### Class Creation\n",
    "\n",
    "**AgentState**  \n",
    "\n",
    "    Defines what data flows between nodes in your LangGraph (e.g., messages, tool results).\n",
    "\n",
    "    Keeps the workflow consistent and prevents errors (e.g., wrong data types).\n",
    "\n",
    "    Makes the agent easier to extend later (e.g., add memory, logs).  \n",
    "    \n",
    "\n",
    "**InstructionStep**\n",
    "\n",
    "    Gives a strict schema for step-by-step instructions.\n",
    "\n",
    "    Ensures output is structured and predictable for your frontend.\n",
    "\n",
    "    Makes parsing and rendering simple (no guesswork in how the AI formats steps)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf1fb06b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentState(TypedDict):\n",
    "    \"\"\"Represents the state of the agent's workflow.\"\"\"\n",
    "    # A list of messages, annotated with a reducer to append new messages\n",
    "    messages: Annotated[Sequence[BaseMessage], add_messages]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e96f7af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a type for structured step-by-step instructions\n",
    "class InstructionStep(TypedDict):\n",
    "    # Short heading for the step\n",
    "    title: str\n",
    "    # List of detail lines (each step can have multiple sub-points)\n",
    "    details: List[str]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c3d1c6d",
   "metadata": {},
   "source": [
    "#### Tool Definition\n",
    "\n",
    "SwapAI uses three tools to handle different types of user intents:\n",
    "\n",
    "##### `swap_tokens`\n",
    "\n",
    "* **Purpose:** Handles token swaps on the Base network.\n",
    "* **Example:** `Swap 0.5 ETH to USDC`.\n",
    "* **Parameters:**\n",
    "\n",
    "  * `amount` *(float)* → amount of tokens to swap.\n",
    "  * `from_token` *(str)* → token being swapped from.\n",
    "  * `to_token` *(str)* → token being swapped to.\n",
    "  * `slippage` *(float, default: 1.0)* → allowed price movement during the swap.\n",
    "\n",
    "---\n",
    "\n",
    "##### `send_tokens`\n",
    "\n",
    "* **Purpose:** Transfers tokens to another wallet address.\n",
    "* **Example:** `Send 5 USDC to 0xabc...`.\n",
    "* **Parameters:**\n",
    "\n",
    "  * `amount` *(float)* → amount of tokens to send.\n",
    "  * `token` *(str)* → token being transferred.\n",
    "  * `recipient` *(str)* → wallet address of the recipient.\n",
    "\n",
    "---\n",
    "\n",
    "##### `give_instructions`\n",
    "\n",
    "* **Purpose:** Provides step-by-step guidance in Markdown format.\n",
    "* **Example:** `How do I connect my wallet?`.\n",
    "* **Parameters:**\n",
    "\n",
    "  * `topic` *(str)* → the subject the user needs guidance on.\n",
    "\n",
    "---\n",
    "\n",
    "This structure ensures SwapAI can both execute transactions and educate users depending on their request.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6a9c4847",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tools\n",
    "\n",
    "@tool\n",
    "def swap_tokens(amount: float, from_token: str, to_token: str, slippage: float = 1.0):\n",
    "    \"\"\"Produce a mock swap of tokens following the instructions in 'DEFAULT_SYSTEM_PROMPT'\"\"\"\n",
    "    return {\n",
    "        \"status\": \"success\",\n",
    "        \"tx_hash\": \"0xmockswap123\",\n",
    "        \"details\": f\"Swapped {amount} {from_token} to {to_token} with {slippage}% slippage\"\n",
    "    }\n",
    "\n",
    "# Mock tool to simulate sending tokens\n",
    "@tool\n",
    "def send_tokens(amount: float, token: str, recipient: str):\n",
    "    \"\"\"Produce a mock send transaction following the instructions in 'DEFAULT_SYSTEM_PROMPT '\"\"\"\n",
    "    return {\n",
    "        \"status\": \"success\",\n",
    "        \"tx_hash\": \"0xmocksend456\",\n",
    "        \"details\": f\"Sent {amount} {token} to {recipient}\"\n",
    "    }\n",
    "\n",
    "@tool\n",
    "def give_instructions(\n",
    "    title: str,\n",
    "    summary: str,\n",
    "    steps: List[InstructionStep],\n",
    "    notes: Optional[List[str]] = None,\n",
    "    warnings: Optional[List[str]] = None,\n",
    "    checklist: Optional[List[str]] = None,\n",
    ") -> str:\n",
    "    \"\"\"Generate step-by-step instructions in markdown format with optional notes, warnings, and checklist.\"\"\"\n",
    "    \n",
    "    md = f\"## {title}\\n\"\n",
    "    md += f\"**Short summary:** {summary}\\n\\n\"\n",
    "    md += \"### Steps\\n\"\n",
    "    for i, step in enumerate(steps, 1):\n",
    "        md += f\"{i}. **{step['title']}**\\n\"\n",
    "        for d in step[\"details\"]:\n",
    "            md += f\"   - {d}\\n\"\n",
    "        md += \"\\n\"\n",
    "\n",
    "    if notes:\n",
    "        for n in notes:\n",
    "            md += f\"> **Note:** {n}\\n\"\n",
    "    if warnings:\n",
    "        for w in warnings:\n",
    "            md += f\"> **Warning:** {w}\\n\"\n",
    "    if checklist:\n",
    "        md += \"\\n**Checklist**\\n\"\n",
    "        for c in checklist:\n",
    "            md += f\"✅ {c}\\n\"\n",
    "\n",
    "    return md\n",
    "\n",
    "tools = [swap_tokens, send_tokens, give_instructions]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "095e85df",
   "metadata": {},
   "source": [
    "##### LLM Setup  \n",
    "This block initializes the **ChatGoogleGenerativeAI** model using parameters stored in the `config` file:  \n",
    "\n",
    "- **model** → selects which LLM to use.  \n",
    "- **temperature** → controls randomness in responses.  \n",
    "- **max_output_tokens** → limits the length of generated outputs.  \n",
    "- **system_instruction** → provides a system prompt to guide behavior.  \n",
    "\n",
    "Finally, `.bind_tools(tools)` connects the LLM to our custom tools (`swap`, `send`, `give_instruction`). This allows the model to not only generate text but also take actions by invoking the tools during execution.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7ac6c73b",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=config[\"model\"],\n",
    "    temperature=config[\"temperature\"],\n",
    "    max_output_tokens=config[\"max_output_tokens\"],\n",
    "    model_kwargs={\"system_instruction\": config[\"system_prompt\"]}\n",
    ").bind_tools(tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "29d1381b",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_CONTEXT = 16\n",
    "\n",
    "def swap_agent(state: AgentState) -> AgentState:\n",
    "    # Always include system prompt at the start\n",
    "    system_prompt = SystemMessage(content=DEFAULT_SYSTEM_PROMPT)\n",
    "\n",
    "    # Keep only the last N messages\n",
    "    messages = list(state[\"messages\"])[-MAX_CONTEXT:]\n",
    "\n",
    "    # Call LLM with system prompt + trimmed history\n",
    "    response = llm.invoke([system_prompt] + messages)\n",
    "\n",
    "    # Just return updated messages (response may be normal text or tool call)\n",
    "    return {\"messages\": messages + [response]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fc14dba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def should_continue(state: AgentState):\n",
    "    messages = state[\"messages\"]\n",
    "    last_message = messages[-1]\n",
    "    if not last_message.tool_calls:\n",
    "        return \"end\"\n",
    "    else: \n",
    "        return \"continue\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cf38d748",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new graph with the defined state\n",
    "graph = StateGraph(AgentState)\n",
    "\n",
    "# Add nodes\n",
    "graph.add_node(\"swap_agent\", swap_agent)\n",
    "\n",
    "tool_node = ToolNode(tools=tools)\n",
    "graph.add_node(\"tools\", tool_node)\n",
    "\n",
    "# Set entry point\n",
    "graph.set_entry_point(\"swap_agent\")\n",
    "\n",
    "# Define edges\n",
    "\n",
    "# The llm node can go to tool or end based on the condition\n",
    "graph.add_conditional_edges(\n",
    "    \"swap_agent\",\n",
    "    should_continue,\n",
    "    {\n",
    "        \"continue\": \"tools\",   # if tool call, go to tools\n",
    "        \"end\": END     # else just end (LLM reply is final)\n",
    "    },\n",
    ")\n",
    "\n",
    "\n",
    "graph.add_edge(\"tools\", \"swap_agent\")\n",
    "\n",
    "# compile the graph\n",
    "app = graph.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5021c3b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_with_agent(user_input: str, history=None):\n",
    "    if history is None:\n",
    "        history = []\n",
    "    \n",
    "    history.append(HumanMessage(content=user_input))\n",
    "    state = {\"messages\": history}\n",
    "    result = app.invoke(state)\n",
    "    history.extend([m for m in result[\"messages\"] if isinstance(m, AIMessage)])\n",
    "    \n",
    "    return history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6fae0505",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI: I can help you swap tokens or send tokens on the Base network. Just tell me what you need! \n",
      "AI: Got it! I can help with that.\n",
      "\n",
      "What is the amount of USDC you'd like to send, and what's the recipient's wallet address?\n",
      "AI: I've sent 5 USDC to 0xcccgtksfrj. You can view the transaction details [here](https://example.com/tx/0xmocksend456).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kamiye\\Desktop\\swap-agent\\.venv\\Lib\\site-packages\\langchain_google_genai\\chat_models.py:2007: UserWarning: HumanMessage with empty content was removed to prevent API error\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI:  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kamiye\\Desktop\\swap-agent\\.venv\\Lib\\site-packages\\langchain_google_genai\\chat_models.py:2007: UserWarning: HumanMessage with empty content was removed to prevent API error\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kamiye\\Desktop\\swap-agent\\.venv\\Lib\\site-packages\\langchain_google_genai\\chat_models.py:2007: UserWarning: HumanMessage with empty content was removed to prevent API error\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI: I'm sorry, I cannot fulfill this request. The transaction could not be completed. Please try again.\n"
     ]
    }
   ],
   "source": [
    "history = []\n",
    "while True:\n",
    "    user_input = input(\"You: \")\n",
    "    if user_input.lower() in [\"exit\", \"quit\"]:\n",
    "        break\n",
    "    history = chat_with_agent(user_input, history)\n",
    "    print(\"AI:\", history[-1].content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6252f12",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
