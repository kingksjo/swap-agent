{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "366d9b7a",
   "metadata": {},
   "source": [
    "SwapAI is an AI-powered agent that helps users perform token swaps and transfers on the Base blockchain. It takes natural language requests like *“Swap 0.5 ETH to USDC with 1% slippage”* or *“Send 20 USDC to 0xabc…”*, extracts the parameters, and executes the right action using integrated tools. The agent can also handle general user questions beyond swaps and sends.\n",
    "\n",
    "\n",
    "#### **The goal**\n",
    "\n",
    "* Make blockchain interactions easier through conversational AI.\n",
    "* Ensure clarity by confirming parameters before execution.\n",
    "* Support structured outputs (markdown) for front-end rendering.\n",
    "* Balance between tool usage (for swaps/sends) and general reasoning (for normal questions).\n",
    "\n",
    "---\n",
    "\n",
    "#### **Stack**\n",
    "\n",
    "* **Python + Jupyter Notebook** (development & testing)\n",
    "* **LangChain** (LLM orchestration, tools binding)\n",
    "* **LangGraph** (state management, branching logic, tool handling)\n",
    "* **Google Generative AI (Gemini)** or **OpenAI GPT** (LLM backbone)\n",
    "* **dotenv** (manage API keys and environment variables)\n",
    "\n",
    "---\n",
    "\n",
    "#### **Why LangChain**\n",
    "\n",
    "* Provides a clean interface to connect LLMs with tools.\n",
    "* Offers message-based state management through LangGraph.\n",
    "* Makes it easier to maintain context windows and structured workflows.\n",
    "* Prebuilt utilities like `ToolNode` and `add_messages` reduce boilerplate.\n",
    "* Extensible: easy to add new tools, memory, or different LLM backends.\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "637c43d3",
   "metadata": {},
   "source": [
    "#### Imports\n",
    "\n",
    "1. ***from system_prompt import DEFAULT_SYSTEM_PROMPT***  \n",
    "This imports the DEFAULT_SYSTEM_PROMPT from the system_prompt.py file in the repo.\n",
    "\n",
    "2. ***from dotenv import load_dotenv***  \n",
    "Loads environment variables from a .env file into your Python environment. This lets us keep API keys outside the code in a .env file with *GOOGLE_API_KEY=your API key* as its content\n",
    "\n",
    "3. ***from typing import Annotated, Sequence, List, Optional***    \n",
    "The typing module is used for type hints.  \n",
    "\n",
    "    **Annotated** adds metadata to a type.  \n",
    "\n",
    "    **Sequence** means any ordered collection.    \n",
    "\n",
    "    **List** is specifically a python list.  \n",
    "\n",
    "    **Optional** means the value can be None.   \n",
    "\n",
    "4. ***from typing_extensions import TypedDict***  \n",
    "    **TypedDict** lets you define dictionary “schemas” with fixed keys and types.\n",
    "\n",
    "5. ***from langgraph.graph import StateGraph, END***  \n",
    "\n",
    "    **StateGraph** is the main component for building a state machine or a flowchart for an AI application.\n",
    "\n",
    "    **END** is a special, built-in node that signifies the end of a process.\n",
    "\n",
    "    StateGraph is used to orchestrate complex, multi-step workflows for an AI agent, and END is how you tell the workflow to stop.\n",
    "\n",
    "6. ***from langchain_core.messages import AIMessage, HumanMessage, SystemMessage***  \n",
    "These are data structures that represent the different roles in a conversation. They provide the structured format that chat models require to understand the flow and context of a conversation.  \n",
    "\n",
    "    **BaseMessage:** The base class for all chat messages in LangChain.  \n",
    "\n",
    "    **Subclasses:** \n",
    "\n",
    "        HumanMessage: Represents input from the end-user. \n",
    "\n",
    "        AIMessage: Represents a response generated by the AI model.\n",
    "\n",
    "        SystemMessage: Sets the overall behavior, persona, or high-level instructions for the AI. \n",
    "\n",
    "        ToolMessage: Stores a tool's response. Needed so the conversation history includes not just what the AI says, but also what tools returned.\n",
    "\n",
    "7. ***from langchain_google_genai import ChatGoogleGenerativeAI***  \n",
    "This is the LangChain wrapper for Google's Gemini family of models. It allows the code to commmunicate with the Gemini API in a standardised way that fits into the LangChain ecosystem.\n",
    "\n",
    "8. ***from langchain_core.tools import tool***  \n",
    "This turns a python function into a tool that the AI can use. \n",
    "\n",
    "9. ***from langgraph.graph.message import add_messages***  \n",
    "    **add_messages** is a reducer function. This means: every time a node returns new messages, instead of replacing the old ones, it appends them to the state automatically.\n",
    "\n",
    "10. ***from langgraph.prebuilt import ToolNode***  \n",
    "    **ToolNode** is a prebuilt node that knows how to call tools. You pass it your tools list, and it manages execution + returning results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f4478529",
   "metadata": {},
   "outputs": [],
   "source": [
    "from system_prompt import DEFAULT_SYSTEM_PROMPT\n",
    "\n",
    "from dotenv import load_dotenv \n",
    "\n",
    "from typing import Annotated, Sequence, List, Optional\n",
    "from typing_extensions import TypedDict\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, SystemMessage, ToolMessage\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.tools import tool\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.prebuilt import ToolNode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8806b6c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()   # This loads the .env file which contains the API key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e3d7b9b",
   "metadata": {},
   "source": [
    "#### Config Dictionary  \n",
    "Defines system prompt, creativity (temperature), response length, and model.  \n",
    "\n",
    "Keeps settings in one place → easier to tweak, cleaner code, and easier to switch models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "80070b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"system_prompt\": DEFAULT_SYSTEM_PROMPT,\n",
    "    \"temperature\": 0.9,\n",
    "    \"max_output_tokens\": 1008,\n",
    "    \"model\": \"gemini-2.5-flash-lite\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0774a0e",
   "metadata": {},
   "source": [
    "#### Class Creation\n",
    "\n",
    "**AgentState**  \n",
    "\n",
    "- Defines what data flows between nodes in your LangGraph (e.g., messages, tool results).\n",
    "\n",
    "- Keeps the workflow consistent and prevents errors (e.g., wrong data types).\n",
    "\n",
    "- Makes the agent easier to extend later (e.g., add memory, logs).  \n",
    "    \n",
    "\n",
    "**InstructionStep**\n",
    "\n",
    "- Gives a strict schema for step-by-step instructions.\n",
    "\n",
    "-  Ensures output is structured and predictable for your frontend.\n",
    "\n",
    "- Makes parsing and rendering simple (no guesswork in how the AI formats steps)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cf1fb06b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentState(TypedDict):\n",
    "    \"\"\"Represents the state of the agent's workflow.\"\"\"\n",
    "    # A list of messages, annotated with a reducer to append new messages\n",
    "    messages: Annotated[Sequence[BaseMessage], add_messages]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4e96f7af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a type for structured step-by-step instructions\n",
    "class InstructionStep(TypedDict):\n",
    "    # Short heading for the step\n",
    "    title: str\n",
    "    # List of detail lines (each step can have multiple sub-points)\n",
    "    details: List[str]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c3d1c6d",
   "metadata": {},
   "source": [
    "#### Tool Definition\n",
    "\n",
    "SwapAI uses three tools to handle different types of user intents:\n",
    "\n",
    "##### `swap_tokens`\n",
    "\n",
    "* **Purpose:** Handles token swaps on the Base network.\n",
    "* **Example:** `Swap 0.5 ETH to USDC`.\n",
    "* **Parameters:**\n",
    "\n",
    "  * `amount` *(float)* → amount of tokens to swap.\n",
    "  * `from_token` *(str)* → token being swapped from.\n",
    "  * `to_token` *(str)* → token being swapped to.\n",
    "  * `slippage` *(float, default: 1.0)* → allowed price movement during the swap.\n",
    "\n",
    "---\n",
    "\n",
    "##### `send_tokens`\n",
    "\n",
    "* **Purpose:** Transfers tokens to another wallet address.\n",
    "* **Example:** `Send 5 USDC to 0xabc...`.\n",
    "* **Parameters:**\n",
    "\n",
    "  * `amount` *(float)* → amount of tokens to send.\n",
    "  * `token` *(str)* → token being transferred.\n",
    "  * `recipient` *(str)* → wallet address of the recipient.\n",
    "\n",
    "---\n",
    "\n",
    "##### `give_instructions`\n",
    "\n",
    "* **Purpose:** Provides step-by-step guidance in Markdown format.\n",
    "* **Example:** `How do I connect my wallet?`.\n",
    "* **Parameters:**\n",
    "\n",
    "  * `topic` *(str)* → the subject the user needs guidance on.\n",
    "\n",
    "---\n",
    "\n",
    "This structure ensures SwapAI can both execute transactions and educate users depending on their request.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6a9c4847",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tools\n",
    "\n",
    "@tool\n",
    "def swap_tokens(amount: float, from_token: str, to_token: str, slippage: float = 1.0):\n",
    "    \"\"\"Produce a mock swap of tokens following the instructions in 'DEFAULT_SYSTEM_PROMPT'\"\"\"\n",
    "    return {\n",
    "        \"status\": \"success\",\n",
    "        \"tx_hash\": \"0xmockswap123\",\n",
    "        \"details\": f\"Swapped {amount} {from_token} to {to_token} with {slippage}% slippage\"\n",
    "    }\n",
    "\n",
    "# Mock tool to simulate sending tokens\n",
    "@tool\n",
    "def send_tokens(amount: float, token: str, recipient: str):\n",
    "    \"\"\"Produce a mock send transaction following the instructions in 'DEFAULT_SYSTEM_PROMPT '\"\"\"\n",
    "    return {\n",
    "        \"status\": \"success\",\n",
    "        \"tx_hash\": \"0xmocksend456\",\n",
    "        \"details\": f\"Sent {amount} {token} to {recipient}\"\n",
    "    }\n",
    "\n",
    "@tool\n",
    "def give_instructions(\n",
    "    title: str,\n",
    "    summary: str,\n",
    "    steps: List[InstructionStep],\n",
    "    notes: Optional[List[str]] = None,\n",
    "    warnings: Optional[List[str]] = None,\n",
    "    checklist: Optional[List[str]] = None,\n",
    ") -> str:\n",
    "    \"\"\"Generate step-by-step instructions in markdown format with optional notes, warnings, and checklist.\"\"\"\n",
    "    \n",
    "    md = f\"## {title}\\n\"\n",
    "    md += f\"**Short summary:** {summary}\\n\\n\"\n",
    "    md += \"### Steps\\n\"\n",
    "    for i, step in enumerate(steps, 1):\n",
    "        md += f\"{i}. **{step['title']}**\\n\"\n",
    "        for d in step[\"details\"]:\n",
    "            md += f\"   - {d}\\n\"\n",
    "        md += \"\\n\"\n",
    "\n",
    "    if notes:\n",
    "        for n in notes:\n",
    "            md += f\"> **Note:** {n}\\n\"\n",
    "    if warnings:\n",
    "        for w in warnings:\n",
    "            md += f\"> **Warning:** {w}\\n\"\n",
    "    if checklist:\n",
    "        md += \"\\n**Checklist**\\n\"\n",
    "        for c in checklist:\n",
    "            md += f\"✅ {c}\\n\"\n",
    "\n",
    "    return md\n",
    "\n",
    "tools = [swap_tokens, send_tokens, give_instructions]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "095e85df",
   "metadata": {},
   "source": [
    "##### LLM Setup  \n",
    "This block initializes the `ChatGoogleGenerativeAI` model using parameters stored in the `config` file:  \n",
    "\n",
    "- **model** → selects which LLM to use.  \n",
    "- **temperature** → controls randomness in responses.  \n",
    "- **max_output_tokens** → limits the length of generated outputs.  \n",
    "- **system_instruction** → provides a system prompt to guide behavior.  \n",
    "\n",
    "Finally, `.bind_tools(tools)` connects the LLM to our custom tools (`swap`, `send`, `give_instruction`). This allows the model to not only generate text but also take actions by invoking the tools during execution.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7ac6c73b",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=config[\"model\"],\n",
    "    temperature=config[\"temperature\"],\n",
    "    max_output_tokens=config[\"max_output_tokens\"],\n",
    "    model_kwargs={\"system_instruction\": config[\"system_prompt\"]}\n",
    ").bind_tools(tools)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c89ba5c",
   "metadata": {},
   "source": [
    "#### Swap Agent with Sliding Context Window\n",
    "\n",
    "The `swap_agent` function manages how the agent handles conversations and tool calls and also manages the conversation history for the agent while ensuring it never exceeds a fixed context size (MAX_CONTEXT).  \n",
    "\n",
    "\n",
    "**Key Points**\n",
    "\n",
    "System Prompt: Always prepends the fixed system instruction to maintain agent behavior.\n",
    "\n",
    "Growing History: Adds each new response to the conversation.\n",
    "\n",
    "Sliding Window: Once the number of messages exceeds MAX_CONTEXT, the oldest message is dropped, keeping the latest ones in memory.\n",
    "\n",
    "Stable Memory Size: Ensures the model works with recent context only, avoiding overflow while still maintaining continuity.\n",
    "\n",
    "LLM Invocation: Calls the `llm` with both the system prompt and the message history. The response can be normal text or a tool call.  \n",
    "\n",
    "State Update: Returns a new `AgentState` dictionary. Appends the model’s response to the list of messages.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "29d1381b",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_CONTEXT = 16\n",
    "\n",
    "def swap_agent(state: AgentState) -> AgentState:\n",
    "    system_prompt = SystemMessage(content=DEFAULT_SYSTEM_PROMPT)\n",
    "\n",
    "    # Copy current messages\n",
    "    messages = list(state[\"messages\"])\n",
    "\n",
    "    # Call LLM with system prompt + current history\n",
    "    response = llm.invoke([system_prompt] + messages)\n",
    "\n",
    "    # Enforce MAX_CONTEXT by popping oldest if too long\n",
    "    if len(messages) > MAX_CONTEXT:\n",
    "        messages.pop(0)\n",
    "    \n",
    "    # Add the new response\n",
    "    messages.append(response)\n",
    "\n",
    "    return {\"messages\": messages}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92993b5c",
   "metadata": {},
   "source": [
    "#### should_continue – Control Flow for Tool Usage\n",
    "\n",
    "This function decides whether the agent should continue the conversation flow (e.g., calling a tool) or stop.\n",
    "\n",
    "**Key Points**\n",
    "\n",
    "- Input: Takes the current AgentState, which holds all past messages.\n",
    "\n",
    "- Last Message Check: Looks at the most recent message in the conversation.\n",
    "\n",
    "- Decision Rule: \n",
    "    * If the last message does not request a tool call, the process ends (\"end\").\n",
    "\n",
    "    * If the last message contains a tool call, the agent should continue (\"continue\").\n",
    "\n",
    "**Purpose**\n",
    "\n",
    "This acts as a `router` in the agent’s workflow, ensuring the agent only proceeds with tool execution when necessary, otherwise stopping cleanly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fc14dba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def should_continue(state: AgentState):\n",
    "    messages = state[\"messages\"]\n",
    "    last_message = messages[-1]\n",
    "    if not last_message.tool_calls:\n",
    "        return \"end\"\n",
    "    else: \n",
    "        return \"continue\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055ea618",
   "metadata": {},
   "source": [
    "#### Building the Agent Graph\n",
    "\n",
    "This section defines the workflow graph for the agent using `StateGraph`. The graph manages how the agent moves between reasoning steps (LLM) and tool execution.\n",
    "\n",
    "**Key Steps**\n",
    "\n",
    "- Initialize Graph: A new `StateGraph` is created with `AgentState` as its state schema.\n",
    "\n",
    "- Add Nodes:\n",
    "\n",
    "   * `swap_agent`: Represents the LLM node that handles responses and system prompt context.\n",
    "   * `tools`: A `ToolNode` that wraps all available tools for the agent.\n",
    "\n",
    "- Set Entry Point: The graph starts execution at `swap_agent`, ensuring the agent always reasons first before calling tools.\n",
    "\n",
    "- Conditional Edges: From `swap_agent`, the agent decides what to do using the `should_continue` function:\n",
    "\n",
    "    * \"continue\" → move to `tools` if a tool call is requested.\n",
    "    * \"end\" → stop execution if the reply is final (no tool call).\n",
    "\n",
    "- Regular Edge: After tools are executed, control always flows back to `swap_agent` so the LLM can process results.\n",
    "\n",
    "- Compile Graph: Finally, the graph is compiled into `app`, making it ready for use as the full agent workflow.\n",
    "\n",
    "**Purpose**\n",
    "\n",
    "This setup allows the agent to loop between LLM reasoning and tool use as needed, while also cleanly ending conversations when tool calls aren’t required.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cf38d748",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new graph with the defined state\n",
    "graph = StateGraph(AgentState)\n",
    "\n",
    "# Add nodes\n",
    "graph.add_node(\"swap_agent\", swap_agent)\n",
    "\n",
    "tool_node = ToolNode(tools=tools)\n",
    "graph.add_node(\"tools\", tool_node)\n",
    "\n",
    "# Set entry point\n",
    "graph.set_entry_point(\"swap_agent\")\n",
    "\n",
    "# Define edges\n",
    "\n",
    "# The llm node can go to tool or end based on the condition\n",
    "graph.add_conditional_edges(\n",
    "    \"swap_agent\",\n",
    "    should_continue,\n",
    "    {\n",
    "        \"continue\": \"tools\",   # if tool call, go to tools\n",
    "        \"end\": END     # else just end (LLM reply is final)\n",
    "    },\n",
    ")\n",
    "\n",
    "\n",
    "graph.add_edge(\"tools\", \"swap_agent\")\n",
    "\n",
    "# compile the graph\n",
    "app = graph.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1781445a",
   "metadata": {},
   "source": [
    "#### Chat Interface Function\n",
    "\n",
    "**Function**: `chat_with_agent`\n",
    "\n",
    "This function provides a simple interface for interacting with the agent.\n",
    "\n",
    "**How it Works**\n",
    "\n",
    "- Initialize History: If no conversation history is provided, a new empty list is created.\n",
    "\n",
    "- Add User Input: The user’s message is wrapped as a `HumanMessage` and appended to history.\n",
    "\n",
    "- Prepare State: The current conversation history is packaged into a `state` dictionary for the agent.\n",
    "\n",
    "- Invoke Agent: The compiled agent graph (`app`) is called with the state.\n",
    "\n",
    "- Update History: Any new `AIMessage` responses from the agent are extracted and added to history.\n",
    "\n",
    "- Return: The updated conversation history is returned, maintaining context across turns.\n",
    "\n",
    "**Purpose**\n",
    "\n",
    "This function acts as the chat loop, ensuring smooth back-and-forth conversation with memory of past messages.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5021c3b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_with_agent(user_input: str, history=None):\n",
    "    if history is None:\n",
    "        history = []\n",
    "    \n",
    "    # The history list currently holds everything from PREVIOUS runs.\n",
    "    \n",
    "    # Store the user input and set the initial state\n",
    "    history.append(HumanMessage(content=user_input))\n",
    "    state = {\"messages\": history}\n",
    "\n",
    "    # Keep track of the length *before* the run starts\n",
    "    initial_length = len(history)\n",
    "\n",
    "    result = app.invoke(state)\n",
    "\n",
    "    # The new messages usually start with the LLM's response (AIMessage)\n",
    "    # and might include a ToolMessage or a second AIMessage.\n",
    "    new_messages = result[\"messages\"][initial_length:]\n",
    "\n",
    "    # reassign `history` to the full message list from the result.\n",
    "    history = result[\"messages\"] # Reassign history to the full, updated list\n",
    "    \n",
    "    # Return the newly generated messages, which is what you want to output\n",
    "    return history, new_messages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc4864c",
   "metadata": {},
   "source": [
    "#### Interactive Chat Loop\n",
    "\n",
    "This code creates a continuous chat session between the user and the AI agent. It allows for multi-turn conversations that maintain context until the user decides to exit.\n",
    "\n",
    "**How It Works**\n",
    "\n",
    "- Initialize History: `history = []` starts an empty list to store conversation messages.\n",
    "\n",
    "- Start Chat Loop: The `while True` loop keeps the chat running indefinitely.\n",
    "\n",
    "- User Input: The program waits for user input via `input(\"You: \")`.\n",
    "\n",
    "- Exit Condition: If the user types `\"exit\"` or `\"quit\"`, the loop breaks and the chat ends.\n",
    "\n",
    "- Agent Response: The `chat_with_agent` function is called with the user’s input and chat history. The returned `history` includes both the user’s and AI’s messages.\n",
    "\n",
    "- Display User Message and AI Reply\n",
    "\n",
    "**Purpose**\n",
    "\n",
    "This loop turns your agent into an interactive chatbot that remembers the last few messages, responds in real time, and ends when the user types *exit* or *quit*.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6fae0505",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You: how do i connect my wallet\n",
      "AI: To connect your wallet, you'll typically need to:\n",
      "\n",
      "1.  **Have a compatible wallet:** Make sure you have a wallet like MetaMask, Coinbase Wallet, or another Base-compatible wallet installed.\n",
      "2.  **Visit a dApp:** Go to a decentralized application (dApp) on Base that requires wallet connection.\n",
      "3.  **Click \"Connect Wallet\":** Look for a button that says \"Connect Wallet\" or similar, usually in the top right corner of the dApp.\n",
      "4.  **Select your wallet:** Choose your wallet provider from the options.\n",
      "5.  **Approve connection:** Your wallet will pop up asking for permission to connect to the dApp. Approve the request.\n",
      "\n",
      "Once connected, the dApp will be able to interact with your wallet for transactions.\n",
      "---------------------------------------------\n",
      "\n",
      "You: i want to connect my wallet, can you guide me \n",
      "AI: I can guide you through the general steps of connecting your wallet to a dApp on Base. Could you tell me which wallet you are using (e.g., MetaMask, Coinbase Wallet, etc.)? That will help me give you more specific instructions.\n",
      "---------------------------------------------\n",
      "\n",
      "You: metamask\n",
      "AI: Great choice! Connecting MetaMask to a dApp on Base is usually straightforward. Here's how you can do it:\n",
      "\n",
      "1.  **Ensure MetaMask is set up:** Make sure you have MetaMask installed as a browser extension or mobile app and that you've set up your account.\n",
      "2.  **Add Base Network to MetaMask:** If you haven't already, you'll need to add the Base network to your MetaMask.\n",
      "    *   Open MetaMask.\n",
      "    *   Click on the network dropdown (it might say \"Ethereum Mainnet\" by default).\n",
      "    *   Click \"Add Network\".\n",
      "    *   Manually enter the Base network details:\n",
      "        *   Network Name: Base\n",
      "        *   New RPC URL: '''https://mainnet.base.org'''\n",
      "        *   Chain ID: 8453\n",
      "        *   Currency Symbol: ETH\n",
      "        *   Block Explorer URL: '''https://basescan.org'''\n",
      "    *   Click \"Save\".\n",
      "3.  **Visit a dApp:** Go to the website of the decentralized application on Base you want to use.\n",
      "4.  **Find the \"Connect Wallet\" button:** This is usually prominent on the dApp's homepage, often in the top-right corner.\n",
      "5.  **Select MetaMask:** When prompted, choose \"MetaMask\" as your wallet provider.\n",
      "6.  **Approve the connection:** Your MetaMask extension or mobile app will open, asking you to confirm the connection to the website. Review the permissions and click \"Approve\" or \"Connect.\"\n",
      "\n",
      "Once you've done this, your MetaMask wallet will be connected to the dApp, and you'll see your wallet address usually displayed on the dApp interface. You're all set to start transacting on Base!\n",
      "---------------------------------------------\n",
      "\n",
      "You: thank you\n",
      "AI: You're very welcome! If you have any more questions or need help with anything else on Base, just ask. Happy to assist!\n",
      "---------------------------------------------\n",
      "\n",
      "You: what's your name\n",
      "AI: I'm Miye, your helpful and charismatic assistant for all things tokens on Base! How can I help you today?\n",
      "---------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "history = []\n",
    "while True:\n",
    "    user_input = input(\"You: \")\n",
    "    if user_input.lower() in [\"exit\", \"quit\"]:\n",
    "        break\n",
    "    history, new_messages = chat_with_agent(user_input, history)\n",
    "    print(f\"You: {user_input}\") # Display the user's input\n",
    "    \n",
    "    # Iterate and display all new messages generated by the graph run\n",
    "    for msg in new_messages:\n",
    "        # Determine the prefix based on message type\n",
    "        prefix = \"AI:\"\n",
    "        if isinstance(msg, HumanMessage):\n",
    "            # This should rarely happen but is a good safeguard\n",
    "            prefix = \"Human:\" \n",
    "        \n",
    "        # Print the message content\n",
    "        print(f\"{prefix} {msg.content}\")\n",
    "    \n",
    "    # Add a separator for the next turn\n",
    "    print(\"---------------------------------------------\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea1cba66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e6252f12",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
